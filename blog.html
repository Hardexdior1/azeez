<!DOCTYPE html>
<html>
<head>
<metaname="UTF-8">
<title>My first project</title>
<link rel="stylesheet" href="main.css">
</head>
<body>

<header>
<div class="myboss"><h1>SAMSUNG</h1></div>

<div id="boss">
<nav>
<ul>
<li><a href="html">Home</a></li>
<li><a href="more.html ">About</a></li>
<li><a href="team.html">More</a></li>
<li><a href="#">Blog</a></li>
<li><a href="contact.html">Contact</a></li>
</ul>
</nav>
</div>
       </header>

<div id="yemen">
        
<h3 style="color:white;font-family:serif; font-size:6rem;padding-top:350px;text-align:center;padding-bottom:60px">  Need our help ? 
 </h3>
<p style="color:white; font-size:3em;text-align:center;"> 
Call us
The Tech Platform That Gives Back To The Community. Join the Lucky Winners
 </p>
<h4 style="font-size:3em;text-align:center;padding-top:165px;"><a href="#" style="color:tomato;text-decoration:none;">Learn More</a></h4>
<br>
<br>
<br>
</div>

      <br>
       <br>
       <br>

<div class="big">
<div id="bae">
<img src="image/sc.JPG" width="800" height="500">
<h5>UNCATEGORIZE</h5>
<br>
<br>
<br>
<h1>Samsung Electronics Signs Net Zero Home Cooperation partnership With Solaredge</h1>
<br>
<p style="color:lightgray; float:left;">june 16,2023</p>
<br>
<br>
<p>Samsung Electronics today announced that it has signed a partnership with SolarEdge, the global leader in residential solar and storage solutions, to expand its coverage of the Net Zero Home ecosystem.

Integrating with SolarEdge Home, SolarEdge’s smart energy ecosystem, SmartThings Energy offers users new ways to optimize energy consumption and lower their home’s energy bills. It provides one-stop support, from monitoring a household’s photovoltaic (PV) generation and battery status at a glance to optimize energy consumption and save energy with its AI Energy Mode. The SolarEdge ecosystem has expanded to include Samsung’s wide range of smart energy technologies, including heat pumps to power more devices with clean solar energy, together providing visibility on appliances’ impact on household energy consumption.

Samsung has now created a base for business expansion through this partnership with SolarEdge, a company specializing in smart inverters that convert sunlight into electrical energy. SolarEdge’s innovative energy optimization system uses data-driven automation to make hundreds of optimal energy decisions every day based on smart predictions of solar production, energy consumption patterns, battery status and utility rates.

“The solutions we’ve developed with our key partners underline our commitment to creating a brighter future with solar power,” said Chan-woo Park, Executive Vice President of Digital Appliances Business at Samsung Electronics. “We’re excited to demonstrate how expanding our net zero home solution and SmartThings Energy service will make it easier for consumers to become energy independent, save money and contribute to creating a healthier planet.”

“Samsung is a strategic partner, and we are excited to further expand the SolarEdge Home offering and provide our customers with an expanded suite of possible smart energy appliances,” said Ido Ginodi, VP of Global Product at SolarEdge. “As part of an end-to-end smart energy ecosystem, our smart SolarEdge ONE optimization system will help optimize home energy consumption and lower homeowner energy bills.”

SmartThings Energy will also provide periodic information on carbon intensity as one of its new features, starting at the end of June. Carbon intensity refers to the carbon emissions generated by consuming one kilowatt-hour (kWh) of electricity, and consumers can experience the reduction in carbon emissions through product-specific carbon emission forecasts and participate by choosing relatively low carbon intensity time periods.

In addition, Samsung will expand its Demand Response (DR) service globally starting in the U.S., following its successful introduction in Korea. Backed by local governments, the service provides financial incentives such as cash rewards and credits to consumers if they voluntarily reduce energy use during peak hours of power usage.

Since March, Samsung has been operating DR in cooperation with the Seoul Metropolitan Government and the Korea Electric Power Corporation. Starting at the end of June, Samsung plans to test run DR in California and New York.

Samsung’s collaboration with SolarEdge is not the first of its kind. It has a history of joining hands with a variety of solar energy companies, like Qcells in 2021, and also with SMA Solar Technology and Maxeon Solar Technology.

Samsung will display its innovative, energy-saving solutions at Intersolar Europe 2023 together with its various global partners. The event, Europe’s largest energy exhibition, will take place in Munich, Germany from June 14 to 16.</p>

</div>
<img src="image/sc.JPG" width="800" height="500">
    <p style="font-weight
t:bold; margin:0; padding:0; color:black; font-size:2em;"><em>By Admin</em></p>     </div>


<div class="big">
<div id="bae">
<br>
<br>
<img src="image/boss.JPG" width="800" height="500">
<br>
<h5>UNCATEGORIZE</h5>
<br>
<br>
<h1>Samsung To ShowCase Latest Innovative C-Lab projects At VivaTech 2023</h1>
<br>
<p style="color:lightgray; float:left;">june 16,2023</p>
<br>
<br>
<p>Samsung Electronics today announced it will be showcasing innovative projects developed through its C-Lab program at VivaTech 2023. VivaTech, Europe’s largest startup and tech event, will take place from June 14 to 17 at the Paris Expo Porte de Versailles exhibition and conference center.

Samsung will set up a C-Lab exhibition space in the “K-Startup Hall”1 to display one project from C-Lab Inside, an internal venture program for employees, and four startups fostered by C-Lab Outside, a program for external startups.

C-Lab projects and startups will be able to gauge the global market response and strengthen their business case in front of a large number of visitors at VivaTech. They can also take part in investment and business cooperation consultations on site.
<br>
<h2>Global Launch of Relumino Glass Brings Significant Improvements in Usability and Convenience</h2>
<br>
<img src="image/view.JPG" width="700" height="400">
<p style="color:lightgray; float:left;">C-Lab Inside "Relumino"</p>
<br>

<p>Relumino, a visual aid solution for people with low vision, was selected as a C-Lab Inside project in 2016 and has since been further developed and perfected by Samsung Research.

The solution, whose name means “to give light back,” consists of the Relumino app, a smartphone image processing application that utilizes the residual vision of people with low vision to improve object recognition, and Relumino Glasses, a glasses-type wearable device.

Having previously showcased its Relumino solution at MWC 2017 and CES 2018, Samsung will introduce Relumino Glasses to the global market at VivaTech 2023 and demonstrate how it enhanced the technology’s user experience to provide greater comfort and prevent fatigue.

“As the 2024 Paralympic Games will be held in Paris, France next year, there will be a lot of interest in apps and services for people with disabilities,” said Junghoon Cho, one of the Samsung researchers who developed Relumino. “This exhibition will be an opportunity for Relumino to take another step forward.”</p>
<br>
<h2>Showcasing C-Lab Outside Startups in the Fields of AI, the Metaverse and More</h2>
<br>
<p>Created in October 2018, C-Lab Outside is a startup acceleration program launched to invigorate the startup ecosystem in Korea. Startups enrolled in the C-Lab Outside program are provided with office workspaces, expert mentoring from Samsung employees, digital marketing and financial consulting. Additional support includes potential partnerships with Samsung and opportunities to participate in IT exhibitions such as CES, VivaTech and KES (Korea Electronics Show).

Since the beginning of this year, Samsung has been accelerating the expansion of the domestic startup ecosystem by introducing a series of C-Lab Outside programs. Through these efforts, the program is expected to revitalize the local economy, create high-quality jobs and further contribute to the balanced development of Korea.

At VivaTech 2023, Samsung will support the exhibition of selected companies not only from C-Lab Outside Seoul, but also Daegu and Gwangju, giving local startups a chance to expand into the global market.

“Thanks to Samsung Electronics’ support for VivaTech, we are one step closer to entering Europe,” said Nayul Kim, CEO of CLIKA, a startup that was selected for C-Lab Outside Gwangju. “We will continue to promote the excellence of Korean startups.”

The companies participating in VivaTech were selected from a pool of C-Lab Outside startups that are aiming to enter the European market. The four chosen startups include:</p>
<br>
<div style="font-size:4em; text-align:center;font-family:ariel;">
<ul style="padding-left:70px; list-style-type:disc; color:lightgray;">
<li><p>
NdotLight — creator of a web-based 3D design solution</p></li>
<li><p>Vsion — maker of super clear PDLC film and reverse mode PDLC film</p></li>
<li><p>QuantumCat — the first company to commercialize gold nanocatalyst technology</p></li>
<li><p>CLIKA — provider of an all-in-one Auto-TinyAI platform that helps companies commercialize AI models quickly and reliably</p></li>
</ul>
      </div>
<br>
<h2>Fostering 866 Startups and Projects Over the Last 10 Years</h2>
<br>
<p>Samsung operates C-Lab to create sustainable innovations and contribute to the revitalization of Korea’s domestic startup ecosystem.

Launched in December 2012, C-Lab Inside nurtures employees’ innovative ideas while instilling a corporate culture that puts creativity at the fore. The program supports the development of ideas from all areas of business. Leveraging the success of the C-Lab Inside initiative, C-Lab Outside has been expanding Samsung’s support for new ventures to startups and innovations outside of the Samsung network since 2018.

The company has nurtured a total of 866 startups and projects to date, including 475 through C-Lab Outside and 391 through C-Lab Inside</p>

<img src="image/xuz.JPG" width="100%">


<p style="color:lightgray; float:left;">c-Lab Outside "CLIKA</p>

<img src="image/cus.JPG" width="600" height="400">
  <p style="font-weight
t:bold; margin:0; padding:0; color:black; font-size:2em;"><em>By Admin</em></p> 
</div>
      </div>








 <br>
       <br>
       <br>

<div class="big">
<div id="bae">
<img src="image/roll.PNG" width="800" height="500">
<h5>UNCATEGORIZE</h5>
<br>
<br>
<br>
<h1><a href="#">[CVPR Series #6] LASP: Text-To-Text Optimization For Language-Aware Soft Prompting Of Visions & Language Models</a></h1>
<p style="color:lightgray; float:left;">june 16,2023</p>

<p>The Computer Vision and Pattern Recognition Conference (CVPR) is a world-renowned international Artificial Intelligence (AI) conference co-hosted by the Institute of Electrical and Electronics Engineers (IEEE) and the Computer Vision Foundation (CVF) which has been running since 1983. CVPR is widely considered to be one of the three most significant international conferences in the field of computer vision, alongside the International Conference on Computer Vision (ICCV) and the European Conference on Computer Vision (ECCV).In this relay series, we are introducing a summary of the 7 research papers at the CVPR 2023 and here is a summary of them.– Part 1 : SPIn-NeRF: Multiview Segmentation and Perceptual Inpainting with Neural Radiance Fields (by Samsung AI Center – Toronto)- Part 2 : StepFormer: Self-supervised Step Discovery and Localization in Instructional Videos (by Samsung AI Center – Toronto)– Part 3 : GENIE: Show Me the Data for Quantization (by Samsung Research)– Part 4 : A Unified Pyramid Recurrent Network for Video Frame Interpolation (By Samsung R&D Institute – Nanjing)– Part 5 : MobileVOS: Real-Time Video Object Segmentation Contrastive Learning Meets Knowledge Distillation (By Samsung R&D Institute United Kingdom)– Part 6 : LASP: Text-to-Text Optimization for Language-Aware Soft Prompting of Vision & Language Models (By Samsung AI Center – Cambridge)</p>
<br>
<h2>Introduction</h3>
<br>
<p>Welcome to our research blog post, where we delve into the fascinating world of vision and language models. In recent years, large-scale pre-training of neural networks has paved the way for ground-breaking advancements in Vision & Language (V&L) understanding. These pre-trained models, such as BERT [5] and CLIP [1], have demonstrated their ability to capture the intricacies of the world, enabling them to adapt seamlessly to new tasks and datasets.

In this blog post, we focus on the remarkable potential of V&L models trained with contrastive learning, which have opened doors to few-shot and even zero-shot adaptation. By leveraging the power of contrastive learning, these models can quickly adapt to new downstream tasks with minimal training examples. Prompt engineering and learning have emerged as powerful techniques for adapting V&L models to novel tasks, drawing inspiration from their counterparts in Natural Language Processing (NLP). Initially, researchers relied on manual templates or prompts to create class-specific weights for zero-shot recognition. However, recent advancements have introduced the concept of “soft prompts” [2,3] – learnable vectors that are fed into the text encoder along with the class name. These soft prompts are learned from a few training examples with the entire V&L model kept frozen. The whole process can be seen as parameter efficient fine-tuning of the model on a small training dataset.

Despite the promising results of soft prompt learning, there is a noticeable challenge known as base class overfitting. While accuracy on the base classes improves significantly, the accuracy on unseen novel classes suffers. This issue arises because soft prompts are learned from only a few examples belonging to the base classes. Interestingly, hand-engineered prompts still outperform existing soft prompt learning methods when it comes to recognizing novel classes.</p>
<br>  
<h2>Our Approach</h2>
<p>To address the problem of base class overfitting, we propose a simple, yet highly effective solution motivated by a keen observation: Prompt learning enhances accuracy on base classes, while prompt engineering excels in recognizing novel classes. Drawing on this insight, we introduce a novel text-to-text loss that enforces the learned prompts to be close, in embedding space, to the textual prompts. By exploiting the intrinsic information captured by the text encoder, we enable language-only optimization for V&L model adaptation, a unique approach compared to previous soft prompt learning methods that mainly focus on V&L interactions.</p>
<br>
<h2>Our Contributions</h3>
<br>
<p>We propose a novel framework for soft prompt learning which we call Language-Aware Soft Prompting (LASP). Our main contributions within the LASP framework are as follows:</p>

<div style="font-size:1em; text-align:center;font-family:ariel;">
<ol style="padding-left:50px; color:lightgray;font-weight:bold;">

<li><p>Grouped Language-Aware Prompt Representation:To increase the representation capacity of prompts, we draw inspiration from grouped convolution and multi-head attention. We introduce a grouped language-aware prompt representation, where each group of prompts specializes in a different subset of pre-defined manual templates.</p></li>
 
 <li><p>Addressing Visual-Language Misalignment: Prompt learning and more generally, contrastive pre-training, introduce a visual-language misalignment that impacts generalization. To tackle this challenge, we propose a re-calibration mechanism, which involves Layer Normalization fine-tuning and learning a class-agnostic bias.</p></li>
 <li><p>Training with Virtual Classes:Leveraging our language-only learning framework, we propose training LASP with virtual classes, even when visual samples are unavailable. This strategy further enhances the robustness of the learned prompts.</p></li>
</ol>
    </div>

<p>Through extensive experiments, we showcase the superiority of our approach over existing soft prompting methods. Our methods set a new state-of-the-art for few-shot and zero-shot image classification on multiple datasets. Notably, we present a prompt learning method that outperforms the strong baseline of hand-crafted prompts and CLIP for recognizing novel classes.</p>

<img src="image/roll.PNG" width="800" height="500">

<br>
<h2>Language-Only Optimisation: Language-Aware Soft Prompting (LASP)</h2>
<br>
<p>Our method, called Language-Aware Soft Prompting (LASP) aims to enhance the generalisation and robustness of few-shot adaptation in Vision & Language (V&L) models. Unlike previous methods that primarily focus on V&L interactions, LASP leverages language-only optimization to improve generalization and mitigate base-class overfitting.
<br>
As such, in addition to the vision-language classification loss, we introduce a second cross-entropy loss function that minimizes the distance between learned soft prompts and hand-engineered textual prompts. The textual prompts, obtained by encoding class-specific templates, act as class weights in the language space. This loss encourages the learnable prompts to be correctly classified based on the textual prompts. This support hard prompts can be randomly constructed or formed simply by taking the ones used in [1]. During training, we then calculate the probability of a learnable prompt being classified as a specific class. This probability calculation is based on measuring the cosine similarity between the encoded textual prompt for the target class and the encoded learnable prompt in the embedding space of CLIP’s text encoder.
<br>
he overall training objective is then a combination of the V&L loss and the language-aware loss, weighted by user-defined scaling coefficients. This combined objective ensures that the model learns both visual and language cues effectively, leading to improved performance on novel classes and reduced overfitting to base classes. The overall framework is depicted in Figure 1.
<br>
Intuitively, our method can be interpreted in several ways: as a regularizer, as a language-based augmentation and as a data-free distillation.
<br>
LASP as a regularizer: As the proposed loss encourages the learned prompts to be close in the embedding space to the textual ones, our method can be viewed a s a regularizer that prevents the leaned prompt-conditioned features form diverging too much from the hand-crafted ones.
<br>
LASP as a language-based augmentation:The current best practice for learning from visual data is to randomly apply a set of transformations (such as rotation, scaling etc.) at train time. Our text-to-text optimisation opens the door for a language-based augmentation for V&L adaptation too. In practice, we can achieve this by tareted prompting, where we can specify certain characteristics and/or apply text-based transformations to the class name, e.g.: “A sketch of dog” or “A rotated photo of a dog”.
<br>
LASP as a data-free distillation:Typically, knowledge distillation requires a training set of images, where a teacher network provides a training signal for the student. LASP’s text-to-text loss can be also interpreted as a data-free distillation (i.e. does not use any image data) where the learnable prompts define the “samples”. As CLIP learns a joint V&L space, similar concepts are close together across both domains. Hence, optimizing against a concept or object in the language domain, using the proposed loss, should also help make a step in the visual domain, improving the classification of the images. Furthermore, LASP leverages the joint V&L space of CLIP to improve image classification even without using any image data. By optimizing against the textual prompts, LASP effectively distils knowledge from the language domain to enhance the model’s performance in the visual domain.</p>
<br>
<h2>Grouped Language-Aware Prompt Representation</h2>
<p>Building on the success of techniques like grouped convolutions and multi-head attention, we propose a new approach called Grouped Prompt Representation. This approach aims to optimize prompt learning by dividing the set of textual prompts into separate groups, where each group specializes in a specific subset of prompts: similarly, to how grouped convolutions and multi-head attention combine the expertise of individual groups or heads, our grouped prompt representation leverages the specialization of each group to enhance prompt learning
<br>
To create the groups, we evenly split the set of prompts into multiple subsets. Each subset is associated with a specific group and is optimized to capture unique aspects of the prompts. These specialized prompt groups learn transformations tailored to their respective subset of prompts. The model is then trained using an adapted text-to-text loss that extends the original one to incorporate the grouped prompts. This loss ensures that the model accurately predicts the class probabilities based on the prompts from each group.
<br>
During inference, the final prediction is obtained by averaging the similarity scores between each group’s text feature and the visual feature. This aggregation strategy combines the information from different groups to make a robust and comprehensive prediction.</p>
<br>
<h2>Addressing Re-aligning LASP</h2>
<br>
<br>
<p>For some downstream tasks, there might be a discrepancy between the data distribution of the downstream image dataset and the one used during CLIP training. It is crucial to address this data distribution shift in the downstream adaptation process. However, optimizing the visual encoder directly to account for this shift can lead to overfitting on the base classes, where the V&L embeddings are pushed away from the joint space learned by CLIP. Early experiments with visual adapters have shown a negative impact on zero-shot accuracy.
<br>
To overcome this challenge, we propose an alternative approach: fine-tuning the Layer Normalization (LN) of the CLIP encoder. Fine-tuning the LN parameters provides a more robust way to adapt the visual encoder while maintaining alignment with the joint space learned by CLIP. By selectively adjusting the LN parameters, the model can capture the distributional shift without sacrificing zero-shot accuracy. This fine-tuning process helps the model effectively combat data distribution shift during downstream adaptation.
<br>
<br>

After fine-tuning the LN parameters, there is a possibility of misalignment between the visual and language modalities. To address this issue, we propose learning a “correction” at the output of the text encoder in the form of a learnable offset or bias. This offset aims to re-align the two modalities and ensure their compatibility. Specifically, we introduce a learnable vector b, which is summed to the set of weights W of the linear classifier obtained by passing the learned prompts to the text encoder. It is important to note that the learned offset is shared among all classes, allowing it to be readily applied even in the case of novel classes. This approach enables us to effectively correct for V&L misalignment and improve the overall alignment and compatibility between the two modalities.</p>
<br>
<br>
<h2>Training with Virtual Classes (LASP-V)</h2>
<br>
<p>
A direct observation that can be drawn from the text-to-text optimisation loss is that, in practice, we do not have to use only the class names for which we have labelled image data, as the value of LASP is independent of the input image. To this end, we propose to learn the prompts using both annotated image-text pairs and class names outside the base set (for which we have no images available). We call this setting as training LASP with virtual classes. Our setting combines the best of both words: the guidance from the few annotated image samples and the zero-shot generalizability of language-based training. As we will show bellow, LASP with virtual classes can significantly improve the robustness of the prompts learned. We refer to this variant of our method in the tables bellow as LASP-V.</p>
<br>
<br>

<h2>Experimental Results</h2>
<br>
<br>
<p>Our method is trained and tested across a suite formed of 12 datasets (i.e., ImageNet, Caltech-101, Oxford Pets, Stanford Cars, Flowers-102, Food-101, FGVC Aircraft, SUN397, DTD, EuroSAT and UCF-101). Each dataset is split into a base and novel set, each containing samples belonging to an equal number of disjoint classes. The training is then performed in a few-shot manner (herein, 16 samples per class) on the base set while the testing on both base and new, separately. As the results from Table 1 show, our method outperforms prior work by a large margin, especially for the new set, where notably, our approach outperforms CLIP with hand crafted prompts, with LASP-V (with virtually classes) setting a new state-of-the-art.</p>
<br>
<br>

<img src="image/fig.PNG" width="850">


<p>Table 1.Aggregated classification accuracy (top-1) across a suite of 12 datasets for the base and new set. H represents the harmonic mean of the two.
<br>
While the improvements vary depending on the distribution of the data, on certain datasets, such as EuroSAT the gains are as large as 17% (see Figure 2).</p>

<img src="image/chart.PNG" width="800" height="500">


<p>Figure 2. Classification accuracy (top-1) on the novel classes subset on EuroSAT, DTD, UCF-101 and SUN397 datasets.</p>
<br>
<br>
<h2>Conclusion</h2>
<br>
<p>In summary, in this work, we introduced LASP – a language aware soft prompting method for V&L adaptation that is shown to outperform prior work by large margin. Our work is the first to propose a text-to-text optimisation framework for vision language adaption, largely alleviating the problem of base-class overfitting. Moreover, we showed that our approach, unlike prior work, is amenable to, including during training, virtual classes, i.e., class names for which no visual samples are available, significantly increasing the robustness of the learned prompts. We hope that LASP/LASP-V will serve as a strong baseline and steppingstone for future works in the area of few-shot adaptation for V&L models.</p>
<br>
<br>

<h2>Link to the paper</h2>
<samp>
https://openaccess.thecvf.com/content/CVPR2023/papers/Bulat_LASP_Text-to-Text_Optimization_for_Language-Aware_Soft_Prompting_of_Vision__CVPR_2023_paper.pdf</samp>
<br>
<br>

<h2>References</h2>

<br>
<p>
[1] Learning transferable visual models from natural language supervision, Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, ICML 2021
<br>
[2] Learning to Prompt for Vision-Language Models, Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu, IJCV 2022

<br>
[3] Conditional Prompt Learning for Vision-Language Models, Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu, CVPR 2022
<br>
[4] Prompt Distribution Learning, Yuning Lu, Jianzhuang Liu, Yonggang Zhang, Yajing Liu, Xinmei Tian, CVPR 2022
<br>
[5] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, NAACL 2</p>

<p style="font-weight
t:bold; margin:0; padding:0; color:black; font-size:2em;"><em>By Admin</em></p> 


 <hr>
<br>
<br>
<br>
<br>
     <p style="color:lightgray; font-size:4vw;text-align:center;padding-bottom:50px;">Expressing our gratitude towards the Samsung community and users, we aim to find more effective ways of acknowledging their support and loyalty.</p>
</div>

<div class="bvn">
<div>
<h1><b>Contact</b></h1>
<p>8900093033</p>
<a href="mailto:odekunlewaris@gmail.com" style="font-size:3vw; color:lightgray;">email@example.com</a>

<p>South Korea</p>
</div>
 <div>
<h1>Pages</h1>
<p>About</p>
<p>Management</p>
</div>

<div>
<h1>Resources</h1>
<p>Careers</p>
</div>

<div>
<h1>Legal</h1>
<p>About</p>
<p>Privacy Policy</p>
<p>Terms And Conditions</p>
</div>

   </div>

<br>
<br>
<br>
<hr>
<br>

<footer>
<p style="font-size:4vw; color:lightgray;">Copyright &copy; 2019-2023 Samsung. All Rights Reserved</p>
<a href="#" style="font-size:3vw;">Back to Top</a>
</footer>

</body>
</html>